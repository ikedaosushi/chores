{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "\n",
    "endpoint = 'http://www.arxiv-sanity.com/toptwtr?timefilter=day'\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "r = session.get(endpoint)\n",
    "\n",
    "script = r.html.find('script')[-1].text\n",
    "\n",
    "script = re.sub('.*var papers = ', '', script)\n",
    "json_ = re.sub('(?<=]);.+', '', script)\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "data = json.loads(json_)\n",
    "\n",
    "data[0].keys()\n",
    "\n",
    "for d in data:\n",
    "    d['id'] = d['pid']\n",
    "    d = {k: v for k, v in d.items() if v}\n",
    "    table.put_item(Item=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'authors', 'category', 'comment', 'img', 'in_library', 'link', 'num_discussion', 'originally_published_time', 'pid', 'published_time', 'rawpid', 'tags', 'title', 'id'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1811.03516v1',\n",
       " '1811.03188v1',\n",
       " '1811.03600v1',\n",
       " '1811.03214v1',\n",
       " '1811.03120v1',\n",
       " '1811.03199v1',\n",
       " '1811.03195v1',\n",
       " '1811.03217v1',\n",
       " '1811.03208v1',\n",
       " '1811.03555v1',\n",
       " '1811.03205v1',\n",
       " '1811.03259v1',\n",
       " '1802.05968v2',\n",
       " '1811.03194v1',\n",
       " '1505.05770v6',\n",
       " '1811.03402v1',\n",
       " '1810.10731v2',\n",
       " '1811.03119v1',\n",
       " '1811.03151v1',\n",
       " '1810.12283v1',\n",
       " '1811.03146v1',\n",
       " '1811.03492v1',\n",
       " '1811.03291v1',\n",
       " '1811.03169v1',\n",
       " '1811.03129v1',\n",
       " '1811.03399v1',\n",
       " '1811.03435v1',\n",
       " '1706.03319v2',\n",
       " '1811.03196v1',\n",
       " '1811.03496v1',\n",
       " '1811.03163v1',\n",
       " '1811.03376v1',\n",
       " '1811.03276v1',\n",
       " '1811.03532v1',\n",
       " '1811.03277v1',\n",
       " '1811.03355v1',\n",
       " '1811.03325v1',\n",
       " '1811.00761v1',\n",
       " '1811.03422v1',\n",
       " '1811.03154v1',\n",
       " '1811.03307v1',\n",
       " '1811.03311v1',\n",
       " '1811.03179v1',\n",
       " '1811.03270v1',\n",
       " '1807.03480v1',\n",
       " '1705.03822v2',\n",
       " '1811.03157v1',\n",
       " '1811.03268v1',\n",
       " '1811.03264v1',\n",
       " '1811.03382v1',\n",
       " '1811.03273v1',\n",
       " '1811.03275v1',\n",
       " '1811.03189v1',\n",
       " '1811.03274v1',\n",
       " '1811.03384v1',\n",
       " '1811.03173v1',\n",
       " '1811.03280v1',\n",
       " '1811.03331v1',\n",
       " '1811.03478v1',\n",
       " '1811.03343v1',\n",
       " '1811.02549v3',\n",
       " '1811.03529v1',\n",
       " '1811.03514v1',\n",
       " '1810.02334v3',\n",
       " '1811.03388v1',\n",
       " '1811.03591v1',\n",
       " '1811.03378v1',\n",
       " '1811.03166v1',\n",
       " '1811.03390v1',\n",
       " '1811.03149v1',\n",
       " '1811.03356v1',\n",
       " '1811.03115v1',\n",
       " '1811.03250v1',\n",
       " '1811.03233v1',\n",
       " '1811.03431v1',\n",
       " '1811.03437v1',\n",
       " '1811.03407v1',\n",
       " '1811.03436v1',\n",
       " '1811.03444v1',\n",
       " '1811.03322v1',\n",
       " '1811.03392v1',\n",
       " '1801.04540v2',\n",
       " '1811.02017v1',\n",
       " '1811.03539v1',\n",
       " '1811.03567v1',\n",
       " '1811.03618v1',\n",
       " '1811.03403v1',\n",
       " '1811.03242v1',\n",
       " '1811.03493v1',\n",
       " '1811.03305v1',\n",
       " '1811.02084v1',\n",
       " '1801.07791v5',\n",
       " '1806.04314v3',\n",
       " '1806.02311v3',\n",
       " '1811.03377v1',\n",
       " '1805.08166v2',\n",
       " '1810.09136v1',\n",
       " '1505.04597v1',\n",
       " '1811.02959v2',\n",
       " '1811.02714v2']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d['pid'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'Learning from demonstration (LfD) is useful in settings where hand-coding\\nbehaviour or a reward function is impractical. It has succeeded in a wide range\\nof problems but typically relies on artificially generated demonstrations or\\nspecially deployed sensors and has not generally been able to leverage the\\ncopious demonstrations available in the wild: those that capture behaviour that\\nwas occurring anyway using sensors that were already deployed for another\\npurpose, e.g., traffic camera footage capturing demonstrations of natural\\nbehaviour of vehicles, cyclists, and pedestrians. We propose video to behaviour\\n(ViBe), a new approach to learning models of road user behaviour that requires\\nas input only unlabelled raw video data of a traffic scene collected from a\\nsingle, monocular, uncalibrated camera with ordinary resolution. Our approach\\ncalibrates the camera, detects relevant objects, tracks them through time, and\\nuses the resulting trajectories to perform LfD, yielding models of naturalistic\\nbehaviour. We apply ViBe to raw videos of a traffic intersection and show that\\nit can learn purely from videos, without additional expert knowledge.',\n",
       "  'authors': ['Feryal Behbahani',\n",
       "   'Kyriacos Shiarlis',\n",
       "   'Xi Chen',\n",
       "   'Vitaly Kurin',\n",
       "   'Sudhanshu Kasewa',\n",
       "   'Ciprian Stirbu',\n",
       "   'João Gomes',\n",
       "   'Supratik Paul',\n",
       "   'Frans A. Oliehoek',\n",
       "   'João Messias',\n",
       "   'Shimon Whiteson'],\n",
       "  'category': 'cs.LG',\n",
       "  'comment': '8 pages, 6 figures',\n",
       "  'img': '/static/thumbs/1811.03516v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03516v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03516v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03516',\n",
       "  'tags': ['cs.LG', 'stat.ML'],\n",
       "  'title': 'Learning from Demonstration in the Wild'},\n",
       " {'abstract': 'We propose a novel mathematical framework to address the problem of\\nautomatically solving large jigsaw puzzles. This problem assumes a large image\\nwhich is cut into equal square pieces that are arbitrarily rotated and shuffled\\nand asks to recover the original image given the transformed pieces. The main\\ncontribution of this work is a theoretically-guaranteed method for recovering\\nthe unknown orientations of the puzzle pieces by using the graph connection\\nLaplacian associated with the puzzle. Iterative application of this method and\\nother methods for recovering the unknown shuffles result in a solution for the\\nlarge jigsaw puzzle problem. This solution is not greedy, unlike many other\\nsolutions. Numerical experiments demonstrate the competitive performance of the\\nproposed method.',\n",
       "  'authors': ['Huroyan Vahan', 'Lerman Gilad', 'Wu Hau-Tieng'],\n",
       "  'category': 'cs.CV',\n",
       "  'comment': '',\n",
       "  'img': '/static/thumbs/1811.03188v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03188v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/7/2018',\n",
       "  'pid': '1811.03188v1',\n",
       "  'published_time': '11/7/2018',\n",
       "  'rawpid': '1811.03188',\n",
       "  'tags': ['cs.CV', 'cs.LG', 'math.NA'],\n",
       "  'title': 'Solving Jigsaw Puzzles By The Graph Connection Laplacian'},\n",
       " {'abstract': 'Recent hardware developments have made unprecedented amounts of data\\nparallelism available for accelerating neural network training. Among the\\nsimplest ways to harness next-generation accelerators is to increase the batch\\nsize in standard mini-batch neural network training algorithms. In this work,\\nwe aim to experimentally characterize the effects of increasing the batch size\\non training time, as measured in the number of steps necessary to reach a goal\\nout-of-sample error. Eventually, increasing the batch size will no longer\\nreduce the number of training steps required, but the exact relationship\\nbetween the batch size and how many training steps are necessary is of critical\\nimportance to practitioners, researchers, and hardware designers alike. We\\nstudy how this relationship varies with the training algorithm, model, and\\ndataset and find extremely large variation between workloads. Along the way, we\\nreconcile disagreements in the literature on whether batch size affects model\\nquality. Finally, we discuss the implications of our results for efforts to\\ntrain neural networks much faster in the future.',\n",
       "  'authors': ['Christopher J. Shallue',\n",
       "   'Jaehoon Lee',\n",
       "   'Joe Antognini',\n",
       "   'Jascha Sohl-Dickstein',\n",
       "   'Roy Frostig',\n",
       "   'George E. Dahl'],\n",
       "  'category': 'cs.LG',\n",
       "  'comment': '',\n",
       "  'img': '/static/thumbs/1811.03600v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03600v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03600v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03600',\n",
       "  'tags': ['cs.LG', 'stat.ML'],\n",
       "  'title': 'Measuring the Effects of Data Parallelism on Neural Network Training'},\n",
       " {'abstract': 'The topic of facial landmark detection has been widely covered for pictures\\nof human faces, but it is still a challenge for drawings. Indeed, the\\nproportions and symmetry of standard human faces are not always used for comics\\nor mangas. The personal style of the author, the limitation of colors, etc.\\nmakes the landmark detection on faces in drawings a difficult task. Detecting\\nthe landmarks on manga images will be useful to provide new services for easily\\nediting the character faces, estimating the character emotions, or generating\\nautomatically some animations such as lip or eye movements.\\n This paper contains two main contributions: 1) a new landmark annotation\\nmodel for manga faces, and 2) a deep learning approach to detect these\\nlandmarks. We use the \"Deep Alignment Network\", a multi stage architecture\\nwhere the first stage makes an initial estimation which gets refined in further\\nstages. The first results show that the proposed method succeed to accurately\\nfind the landmarks in more than 80% of the cases.',\n",
       "  'authors': ['Marco Stricker',\n",
       "   'Olivier Augereau',\n",
       "   'Koichi Kise',\n",
       "   'Motoi Iwata'],\n",
       "  'category': 'cs.CV',\n",
       "  'comment': '',\n",
       "  'img': '/static/thumbs/1811.03214v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03214v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03214v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03214',\n",
       "  'tags': ['cs.CV', 'cs.MM'],\n",
       "  'title': 'Facial Landmark Detection for Manga Images'},\n",
       " {'abstract': 'This paper tackles the challenge of colorizing grayscale images. We take a\\ndeep convolutional neural network approach, and choose to take the angle of\\nclassification, working on a finite set of possible colors. Similarly to a\\nrecent paper, we implement a loss and a prediction function that favor\\nrealistic, colorful images rather than \"true\" ones.\\n We show that a rather lightweight architecture inspired by the U-Net, and\\ntrained on a reasonable amount of pictures of landscapes, achieves satisfactory\\nresults on this specific subset of pictures. We show that data augmentation\\nsignificantly improves the performance and robustness of the model, and provide\\nvisual analysis of the prediction confidence.\\n We show an application of our model, extending the task to video\\ncolorization. We suggest a way to smooth color predictions across frames,\\nwithout the need to train a recurrent network designed for sequential inputs.',\n",
       "  'authors': ['Vincent Billaut', 'Matthieu de Rochemonteix', 'Marc Thibault'],\n",
       "  'category': 'cs.CV',\n",
       "  'comment': '9 pages, 10 figures Stanford University CS231n project',\n",
       "  'img': '/static/thumbs/1811.03120v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03120v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/7/2018',\n",
       "  'pid': '1811.03120v1',\n",
       "  'published_time': '11/7/2018',\n",
       "  'rawpid': '1811.03120',\n",
       "  'tags': ['cs.CV'],\n",
       "  'title': 'ColorUNet: A convolutional classification approach to colorization'},\n",
       " {'abstract': 'Word vector representations are a crucial part of Natural Language Processing\\n(NLP) and Human Computer Interaction. In this paper, we propose a novel word\\nvector representation, Confusion2Vec, motivated from the human speech\\nproduction and perception that encodes representational ambiguity. Humans\\nemploy both acoustic similarity cues and contextual cues to decode information\\nand we focus on a model that incorporates both sources of information. The\\nrepresentational ambiguity of acoustics, which manifests itself in word\\nconfusions, is often resolved by both humans and machines through contextual\\ncues. A range of representational ambiguities can emerge in various domains\\nfurther to acoustic perception, such as morphological transformations,\\nparaphrasing for NLP tasks like machine translation etc. In this work, we\\npresent a case study in application to Automatic Speech Recognition (ASR),\\nwhere the word confusions are related to acoustic similarity. We present\\nseveral techniques to train an acoustic perceptual similarity representation\\nambiguity. We term this Confusion2Vec and learn on unsupervised-generated data\\nfrom ASR confusion networks or lattice-like structures. Appropriate evaluations\\nfor the Confusion2Vec are formulated for gauging acoustic similarity in\\naddition to semantic-syntactic and word similarity evaluations. The\\nConfusion2Vec is able to model word confusions efficiently, without\\ncompromising on the semantic-syntactic word relations, thus effectively\\nenriching the word vector space with extra task relevant ambiguity information.\\nWe provide an intuitive exploration of the 2-dimensional Confusion2Vec space\\nusing Principal Component Analysis of the embedding and relate to semantic,\\nsyntactic and acoustic relationships. The potential of Confusion2Vec in the\\nutilization of uncertainty present in lattices is demonstrated through small\\nexamples relating to ASR error correction.',\n",
       "  'authors': ['Prashanth Gurunath Shivakumar', 'Panayiotis Georgiou'],\n",
       "  'category': 'cs.CL',\n",
       "  'comment': '',\n",
       "  'img': '/static/thumbs/1811.03199v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03199v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03199v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03199',\n",
       "  'tags': ['cs.CL'],\n",
       "  'title': 'Confusion2Vec: Towards Enriching Vector Space Word Representations with\\n Representational Ambiguities'},\n",
       " {'abstract': 'Consider an instance of Euclidean $k$-means or $k$-medians clustering. We\\nshow that the cost of the optimal solution is preserved up to a factor of\\n$(1+\\\\varepsilon)$ under a projection onto a random $O(\\\\log(k / \\\\varepsilon) /\\n\\\\varepsilon^2)$-dimensional subspace. Further, the cost of every clustering is\\npreserved within $(1+\\\\varepsilon)$. More generally, our result applies to any\\ndimension reduction map satisfying a mild sub-Gaussian-tail condition. Our\\nbound on the dimension is nearly optimal. Additionally, our result applies to\\nEuclidean $k$-clustering with the distances raised to the $p$-th power for any\\nconstant $p$.\\n For $k$-means, our result resolves an open problem posed by Cohen, Elder,\\nMusco, Musco, and Persu (STOC 2015); for $k$-medians, it answers a question\\nraised by Kannan.',\n",
       "  'authors': ['Konstantin Makarychev', 'Yury Makarychev', 'Ilya Razenshteyn'],\n",
       "  'category': 'cs.DS',\n",
       "  'comment': '25 pages, 1 figure',\n",
       "  'img': '/static/thumbs/1811.03195v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03195v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03195v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03195',\n",
       "  'tags': ['cs.DS', 'cs.LG'],\n",
       "  'title': 'Performance of Johnson-Lindenstrauss Transform for k-Means and k-Medians\\n Clustering'},\n",
       " {'abstract': 'This paper proposed a novel RGB-D SLAM method for dynamic environments. It\\nfollows traditional feature-based SLAM methods and utilizes a feature groups\\nsegmentation method to resist the disturbance caused by the dynamic objects\\nusing points correlations. The correlations between map points represented with\\na sparse graph are created by Delaunay triangulation. After removing\\nnon-consistency connections, the dynamic objects are separated from static\\nbackground. The features only in the static map are used for motion estimation\\nand bundle adjustment which improves the accuracy and robustness of SLAM in\\ndynamic environments. The effectiveness of the proposed SLAM are evaluated\\nusing TUM RGB-D benchmark. The experiments demonstrate that the dynamic\\nfeatures are successfully removed and the system work perfectly in both low and\\nhigh dynamic environments. The comparisons between proposed method and\\nstate-of-the-art visual systems clearly show that the comparable accurate\\nresults are achieved in low dynamic environments and the performance is\\nimproved significantly in high dynamic environments.',\n",
       "  'authors': ['Weichen Dai', 'Yu Zhang', 'Ping Li', 'Zheng Fang'],\n",
       "  'category': 'cs.CV',\n",
       "  'comment': '11 pages',\n",
       "  'img': '/static/thumbs/1811.03217v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03217v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03217v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03217',\n",
       "  'tags': ['cs.CV', 'cs.RO'],\n",
       "  'title': 'RGB-D SLAM in Dynamic Environments Using Points Correlations'},\n",
       " {'abstract': 'Tree-like structures, such as blood vessels, often express complexity at very\\nfine scales, requiring high-resolution grids to adequately describe their\\nshape. Such sparse morphology can alternately be represented by locations of\\ncentreline points, but learning from this type of data with deep learning is\\nchallenging due to it being unordered, and permutation invariant. In this work,\\nwe propose a deep neural network that directly consumes unordered points along\\nthe centreline of a branching structure, to identify the topology of the\\nrepresented structure in a single-shot. Key to our approach is the use of a\\nnovel multi-task loss function, enabling instance segmentation of arbitrarily\\ncomplex branching structures. We train the network solely using synthetically\\ngenerated data, utilizing domain randomization to facilitate the transfer to\\nreal 2D and 3D data. Results show that our network can reliably extract\\nmeaningful information about branch locations, bifurcations and endpoints, and\\nsets a new benchmark for semantic instance segmentation in branching\\nstructures.',\n",
       "  'authors': ['Kerry Halupka', 'Rahil Garnavi', 'Stephen Moore'],\n",
       "  'category': 'cs.CV',\n",
       "  'comment': 'Accepted to WACV 2019',\n",
       "  'img': '/static/thumbs/1811.03208v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03208v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03208v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03208',\n",
       "  'tags': ['cs.CV'],\n",
       "  'title': 'Deep Semantic Instance Segmentation of Tree-like Structures Using\\n Synthetic Data'},\n",
       " {'abstract': 'We present a novel modular architecture for StarCraft II AI. The architecture\\nsplits responsibilities between multiple modules that each control one aspect\\nof the game, such as build-order selection or tactics. A centralized scheduler\\nreviews macros suggested by all modules and decides their order of execution.\\nAn updater keeps track of environment changes and instantiates macros into\\nseries of executable actions. Modules in this framework can be optimized\\nindependently or jointly via human design, planning, or reinforcement learning.\\nWe apply deep reinforcement learning techniques to training two out of six\\nmodules of a modular agent with self-play, achieving 94% or 87% win rates\\nagainst the \"Harder\" (level 5) built-in Blizzard bot in Zerg vs. Zerg matches,\\nwith or without fog-of-war.',\n",
       "  'authors': ['Dennis Lee',\n",
       "   'Haoran Tang',\n",
       "   'Jeffrey O Zhang',\n",
       "   'Huazhe Xu',\n",
       "   'Trevor Darrell',\n",
       "   'Pieter Abbeel'],\n",
       "  'category': 'cs.AI',\n",
       "  'comment': 'Accepted to The 14th AAAI Conference on Artificial Intelligence and\\n Interactive Digital Entertainm...',\n",
       "  'img': '/static/thumbs/1811.03555v1.pdf.jpg',\n",
       "  'in_library': 0,\n",
       "  'link': 'http://arxiv.org/abs/1811.03555v1',\n",
       "  'num_discussion': 0,\n",
       "  'originally_published_time': '11/8/2018',\n",
       "  'pid': '1811.03555v1',\n",
       "  'published_time': '11/8/2018',\n",
       "  'rawpid': '1811.03555',\n",
       "  'tags': ['cs.AI'],\n",
       "  'title': 'Modular Architecture for StarCraft II with Deep Reinforcement Learning'}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.session import Session\n",
    "\n",
    "session = Session(profile_name='sls')\n",
    "dynamodb = session.resource('dynamodb', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'arxiveSanityRss-dev'\n",
    "table = dynamodb.Table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.scan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Items': [{'img': '/static/thumbs/1811.03233v1.pdf.jpg',\n",
       "   'pid': '1811.03233v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'An activation boundary for a neuron refers to a separating hyperplane that\\ndetermines whether the neuron is activated or deactivated. It has been long\\nconsidered in neural networks that the activations of neurons, rather than\\ntheir exact output values, play the most important role in forming\\nclassification friendly partitions of the hidden feature space. However, as far\\nas we know, this aspect of neural networks has not been considered in the\\nliterature of knowledge transfer. In this paper, we propose a knowledge\\ntransfer method via distillation of activation boundaries formed by hidden\\nneurons. For the distillation, we propose an activation transfer loss that has\\nthe minimum value when the boundaries generated by the student coincide with\\nthose by the teacher. Since the activation transfer loss is not differentiable,\\nwe design a piecewise differentiable loss approximating the activation transfer\\nloss. By the proposed method, the student learns a separating boundary between\\nactivation region and deactivation region formed by each neuron in the teacher.\\nThrough the experiments in various aspects of knowledge transfer, it is\\nverified that the proposed method outperforms the current state-of-the-art.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.LG',\n",
       "   'rawpid': '1811.03233',\n",
       "   'comment': 'Accepted to AAAI 2019',\n",
       "   'link': 'http://arxiv.org/abs/1811.03233v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03233v1',\n",
       "   'tags': ['cs.LG', 'cs.CV', 'stat.ML'],\n",
       "   'title': 'Knowledge Transfer via Distillation of Activation Boundaries Formed by\\n Hidden Neurons',\n",
       "   'authors': ['Byeongho Heo', 'Minsik Lee', 'Sangdoo Yun', 'Jin Young Choi']},\n",
       "  {'img': '/static/thumbs/1811.03444v1.pdf.jpg',\n",
       "   'pid': '1811.03444v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'After the success of deep generative models in image generation tasks,\\nlearning disentangled latent variable of data has become a major part of deep\\nlearning research. Many models have been proposed to learn an interpretable and\\nfactorized representation of latent variable by modifying their objective\\nfunction or model architecture. While disentangling the latent variable, some\\nmodels show lower quality of reconstructed images and others increase the model\\ncomplexity which is hard to train. In this paper, we propose a simple\\ndisentangling method with traditional principle component analysis (PCA) which\\nis applied to the latent variables of variational auto-encoder (VAE). Our\\nmethod can be applied to any generative models. In experiment, we apply our\\nproposed method to simple VAE models and experimental results confirm that our\\nmethod finds more interpretable factors from the latent space while keeping the\\nreconstruction error the same.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.LG',\n",
       "   'rawpid': '1811.03444',\n",
       "   'comment': '5 pages, submitted to ICASSP 2019',\n",
       "   'link': 'http://arxiv.org/abs/1811.03444v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03444v1',\n",
       "   'tags': ['cs.LG', 'stat.ML'],\n",
       "   'title': 'Disentangling Latent Factors with Whitening',\n",
       "   'authors': ['Sangchul Hahn', 'Heeyoul Choi']},\n",
       "  {'img': '/static/thumbs/1811.03276v1.pdf.jpg',\n",
       "   'pid': '1811.03276v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'This paper compares classical copying and quantum entanglement in natural\\nlanguage by considering the case of verb phrase (VP) ellipsis. VP ellipsis is a\\nnon-linear linguistic phenomenon that requires the reuse of resources, making\\nit the ideal test case for a comparative study of different copying behaviours\\nin compositional models of natural language. Following the line of research in\\ncompositional distributional semantics set out by (Coecke et al., 2010) we\\ndevelop an extension of the Lambek calculus which admits a controlled form of\\ncontraction to deal with the copying of linguistic resources. We then develop\\ntwo different compositional models of distributional meaning for this calculus.\\nIn the first model, we follow the categorical approach of (Coecke et al., 2013)\\nin which a functorial passage sends the proofs of the grammar to linear maps on\\nvector spaces and we use Frobenius algebras to allow for copying. In the second\\ncase, we follow the more traditional approach that one finds in categorial\\ngrammars, whereby an intermediate step interprets proofs as non-linear lambda\\nterms, using multiple variable occurrences that model classical copying. As a\\ncase study, we apply the models to derive different readings of ambiguous\\nelliptical phrases and compare the analyses that each model provides.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.CL',\n",
       "   'rawpid': '1811.03276',\n",
       "   'comment': 'In Proceedings CAPNS 2018, arXiv:1811.02701',\n",
       "   'link': 'http://arxiv.org/abs/1811.03276v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03276v1',\n",
       "   'tags': ['cs.CL', 'cs.AI', 'cs.LO'],\n",
       "   'title': 'Classical Copying versus Quantum Entanglement in Natural Language: The\\n Case of VP-ellipsis',\n",
       "   'authors': ['Gijs Wijnholds', 'Mehrnoosh Sadrzadeh']},\n",
       "  {'img': '/static/thumbs/1811.03555v1.pdf.jpg',\n",
       "   'pid': '1811.03555v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'We present a novel modular architecture for StarCraft II AI. The architecture\\nsplits responsibilities between multiple modules that each control one aspect\\nof the game, such as build-order selection or tactics. A centralized scheduler\\nreviews macros suggested by all modules and decides their order of execution.\\nAn updater keeps track of environment changes and instantiates macros into\\nseries of executable actions. Modules in this framework can be optimized\\nindependently or jointly via human design, planning, or reinforcement learning.\\nWe apply deep reinforcement learning techniques to training two out of six\\nmodules of a modular agent with self-play, achieving 94% or 87% win rates\\nagainst the \"Harder\" (level 5) built-in Blizzard bot in Zerg vs. Zerg matches,\\nwith or without fog-of-war.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.AI',\n",
       "   'rawpid': '1811.03555',\n",
       "   'comment': 'Accepted to The 14th AAAI Conference on Artificial Intelligence and\\n Interactive Digital Entertainm...',\n",
       "   'link': 'http://arxiv.org/abs/1811.03555v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03555v1',\n",
       "   'tags': ['cs.AI'],\n",
       "   'title': 'Modular Architecture for StarCraft II with Deep Reinforcement Learning',\n",
       "   'authors': ['Dennis Lee',\n",
       "    'Haoran Tang',\n",
       "    'Jeffrey O Zhang',\n",
       "    'Huazhe Xu',\n",
       "    'Trevor Darrell',\n",
       "    'Pieter Abbeel']},\n",
       "  {'img': '/static/thumbs/1811.03392v1.pdf.jpg',\n",
       "   'pid': '1811.03392v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'The key to success in machine learning (ML) is the use of effective data\\nrepresentations. Traditionally, data representations were hand-crafted.\\nRecently it has been demonstrated that, given sufficient data, deep neural\\nnetworks can learn effective implicit representations from simple input\\nrepresentations. However, for most scientific problems, the use of deep\\nlearning is not appropriate as the amount of available data is limited, and/or\\nthe output models must be explainable. Nevertheless, many scientific problems\\ndo have significant amounts of data available on related tasks, which makes\\nthem amenable to multi-task learning, i.e. learning many related problems\\nsimultaneously. Here we propose a novel and general representation learning\\napproach for multi-task learning that works successfully with small amounts of\\ndata. The fundamental new idea is to transform an input intrinsic data\\nrepresentation (i.e., handcrafted features), to an extrinsic representation\\nbased on what a pre-trained set of models predict about the examples. This\\ntransformation has the dual advantages of producing significantly more accurate\\npredictions, and providing explainable models. To demonstrate the utility of\\nthis transformative learning approach, we have applied it to three real-world\\nscientific problems: drug-design (quantitative structure activity relationship\\nlearning), predicting human gene expression (across different tissue types and\\ndrug treatments), and meta-learning for machine learning (predicting which\\nmachine learning methods work best for a given problem). In all three problems,\\ntransformative machine learning significantly outperforms the best intrinsic\\nrepresentation.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.LG',\n",
       "   'rawpid': '1811.03392',\n",
       "   'published_time': '11/8/2018',\n",
       "   'link': 'http://arxiv.org/abs/1811.03392v1',\n",
       "   'id': '1811.03392v1',\n",
       "   'tags': ['cs.LG', 'stat.ML'],\n",
       "   'title': 'Transformative Machine Learning',\n",
       "   'authors': ['Ivan Olier',\n",
       "    'Oghenejokpeme I. Orhobor',\n",
       "    'Joaquin Vanschoren',\n",
       "    'Ross D. King']},\n",
       "  {'img': '/static/thumbs/1811.03273v1.pdf.jpg',\n",
       "   'pid': '1811.03273v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': \"This paper is about pregroup models of natural languages, and how they relate\\nto the explicitly categorical use of pregroups in Compositional Distributional\\nSemantics and Natural Language Processing. These categorical interpretations\\nmake certain assumptions about the nature of natural languages that, when\\nstated formally, may be seen to impose strong restrictions on pregroup grammars\\nfor natural languages.\\n We formalize this as a hypothesis about the form that pregroup models of\\nnatural languages must take, and demonstrate by an artificial language example\\nthat these restrictions are not imposed by the pregroup axioms themselves. We\\ncompare and contrast the artificial language examples with natural languages\\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\\nan illustrative example).\\n The hypothesis is simply that there must exist a causal connection, or\\ninformation flow, between the words of a sentence in a language whose purpose\\nis to communicate information. This is not necessarily the case with formal\\nlanguages that are simply generated by a series of 'meaning-free' rules. This\\nimposes restrictions on the types of pregroup grammars that we expect to find\\nin natural languages; we formalize this in algebraic, categorical, and\\ngraphical terms.\\n We take some preliminary steps in providing conditions that ensure pregroup\\nmodels satisfy these conjectured properties, and discuss the more general forms\\nthis hypothesis may take.\",\n",
       "   'category': 'cs.CL',\n",
       "   'rawpid': '1811.03273',\n",
       "   'comment': 'In Proceedings CAPNS 2018, arXiv:1811.02701',\n",
       "   'link': 'http://arxiv.org/abs/1811.03273v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03273v1',\n",
       "   'tags': ['cs.CL', 'cs.FL'],\n",
       "   'title': 'Information Flow in Pregroup Models of Natural Language',\n",
       "   'authors': ['Peter M. Hines']},\n",
       "  {'img': '/static/thumbs/1811.03205v1.pdf.jpg',\n",
       "   'pid': '1811.03205v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'We study the problem of learning conditional generators from noisy labeled\\nsamples, where the labels are corrupted by random noise. A standard training of\\nconditional GANs will not only produce samples with wrong labels, but also\\ngenerate poor quality samples. We consider two scenarios, depending on whether\\nthe noise model is known or not. When the distribution of the noise is known,\\nwe introduce a novel architecture which we call Robust Conditional GAN (RCGAN).\\nThe main idea is to corrupt the label of the generated sample before feeding to\\nthe adversarial discriminator, forcing the generator to produce samples with\\nclean labels. This approach of passing through a matching noisy channel is\\njustified by corresponding multiplicative approximation bounds between the loss\\nof the RCGAN and the distance between the clean real distribution and the\\ngenerator distribution. This shows that the proposed approach is robust, when\\nused with a carefully chosen discriminator architecture, known as projection\\ndiscriminator. When the distribution of the noise is not known, we provide an\\nextension of our architecture, which we call RCGAN-U, that learns the noise\\nmodel simultaneously while training the generator. We show experimentally on\\nMNIST and CIFAR-10 datasets that both the approaches consistently improve upon\\nbaseline approaches, and RCGAN-U closely matches the performance of RCGAN.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'stat.ML',\n",
       "   'rawpid': '1811.03205',\n",
       "   'published_time': '11/8/2018',\n",
       "   'link': 'http://arxiv.org/abs/1811.03205v1',\n",
       "   'id': '1811.03205v1',\n",
       "   'tags': ['stat.ML', 'cs.AI', 'cs.LG'],\n",
       "   'title': 'Robustness of Conditional GANs to Noisy Labels',\n",
       "   'authors': ['Kiran Koshy Thekumparampil',\n",
       "    'Ashish Khetan',\n",
       "    'Zinan Lin',\n",
       "    'Sewoong Oh']},\n",
       "  {'img': '/static/thumbs/1811.03516v1.pdf.jpg',\n",
       "   'pid': '1811.03516v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'Learning from demonstration (LfD) is useful in settings where hand-coding\\nbehaviour or a reward function is impractical. It has succeeded in a wide range\\nof problems but typically relies on artificially generated demonstrations or\\nspecially deployed sensors and has not generally been able to leverage the\\ncopious demonstrations available in the wild: those that capture behaviour that\\nwas occurring anyway using sensors that were already deployed for another\\npurpose, e.g., traffic camera footage capturing demonstrations of natural\\nbehaviour of vehicles, cyclists, and pedestrians. We propose video to behaviour\\n(ViBe), a new approach to learning models of road user behaviour that requires\\nas input only unlabelled raw video data of a traffic scene collected from a\\nsingle, monocular, uncalibrated camera with ordinary resolution. Our approach\\ncalibrates the camera, detects relevant objects, tracks them through time, and\\nuses the resulting trajectories to perform LfD, yielding models of naturalistic\\nbehaviour. We apply ViBe to raw videos of a traffic intersection and show that\\nit can learn purely from videos, without additional expert knowledge.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.LG',\n",
       "   'rawpid': '1811.03516',\n",
       "   'comment': '8 pages, 6 figures',\n",
       "   'link': 'http://arxiv.org/abs/1811.03516v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03516v1',\n",
       "   'tags': ['cs.LG', 'stat.ML'],\n",
       "   'title': 'Learning from Demonstration in the Wild',\n",
       "   'authors': ['Feryal Behbahani',\n",
       "    'Kyriacos Shiarlis',\n",
       "    'Xi Chen',\n",
       "    'Vitaly Kurin',\n",
       "    'Sudhanshu Kasewa',\n",
       "    'Ciprian Stirbu',\n",
       "    'João Gomes',\n",
       "    'Supratik Paul',\n",
       "    'Frans A. Oliehoek',\n",
       "    'João Messias',\n",
       "    'Shimon Whiteson']},\n",
       "  {'img': '/static/thumbs/1811.03532v1.pdf.jpg',\n",
       "   'pid': '1811.03532v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'In barter exchanges, participants directly trade their endowed goods in a\\nconstrained economic setting without money. Transactions in barter exchanges\\nare often facilitated via a central clearinghouse that must match participants\\neven in the face of uncertainty---over participants, existence and quality of\\npotential trades, and so on. Leveraging robust combinatorial optimization\\ntechniques, we address uncertainty in kidney exchange, a real-world barter\\nmarket where patients swap (in)compatible paired donors. We provide two\\nscalable robust methods to handle two distinct types of uncertainty in kidney\\nexchange---over the quality and the existence of a potential match. The latter\\ncase directly addresses a weakness in all stochastic-optimization-based methods\\nto the kidney exchange clearing problem, which all necessarily require explicit\\nestimates of the probability of a transaction existing---a still-unsolved\\nproblem in this nascent market. We also propose a novel, scalable kidney\\nexchange formulation that eliminates the need for an exponential-time\\nconstraint generation process in competing formulations, maintains provable\\noptimality, and serves as a subsolver for our robust approach. For each type of\\nuncertainty we demonstrate the benefits of robustness on real data from a\\nlarge, fielded kidney exchange in the United States. We conclude by drawing\\nparallels between robustness and notions of fairness in the kidney exchange\\nsetting.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.AI',\n",
       "   'rawpid': '1811.03532',\n",
       "   'comment': 'Presented at AAAI19',\n",
       "   'link': 'http://arxiv.org/abs/1811.03532v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03532v1',\n",
       "   'tags': ['cs.AI', 'I.2; G.1.6'],\n",
       "   'title': 'Scalable Robust Kidney Exchange',\n",
       "   'authors': ['Duncan C McElfresh', 'Hoda Bidkhori', 'John P Dickerson']},\n",
       "  {'img': '/static/thumbs/1811.03388v1.pdf.jpg',\n",
       "   'pid': '1811.03388v1',\n",
       "   'originally_published_time': '11/8/2018',\n",
       "   'abstract': 'Knowledge tracing is a sequence prediction problem where the goal is to\\npredict the outcomes of students over questions as they are interacting with a\\nlearning platform. By tracking the evolution of the knowledge of some student,\\none can optimize instruction. Existing methods are either based on temporal\\nlatent variable models, or factor analysis with temporal features. We here show\\nthat factorization machines (FMs), a model for regression or classification,\\nencompass several existing models in the educational literature as special\\ncases, notably additive factor model, performance factor model, and\\nmultidimensional item response theory. We show, using several real datasets of\\ntens of thousands of users and items, that FMs can estimate student knowledge\\naccurately and fast even when student data is sparsely observed, and handle\\nside information such as multiple knowledge components and number of attempts\\nat item or skill level. Our approach allows to fit student models of higher\\ndimension than existing models, and provides a testbed to try new combinations\\nof features in order to improve existing models.',\n",
       "   'updated_at': Decimal('1541903625445'),\n",
       "   'category': 'cs.IR',\n",
       "   'rawpid': '1811.03388',\n",
       "   'comment': '8 pages, 7 tables, to appear at the 33th AAAI Conference on\\n Artificial Intelligence (AAAI-19)',\n",
       "   'link': 'http://arxiv.org/abs/1811.03388v1',\n",
       "   'published_time': '11/8/2018',\n",
       "   'id': '1811.03388v1',\n",
       "   'tags': ['cs.IR', 'cs.AI', 'cs.LG', 'stat.ML'],\n",
       "   'title': 'Knowledge Tracing Machines: Factorization Machines for Knowledge Tracing',\n",
       "   'authors': ['Jill-Jênn Vie', 'Hisashi Kashima']}],\n",
       " 'Count': 10,\n",
       " 'ScannedCount': 10,\n",
       " 'LastEvaluatedKey': {'id': '1811.03388v1'},\n",
       " 'ResponseMetadata': {'RequestId': '4NEL4TCFP924TGIQ9CEHBNLQIJVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Sun, 11 Nov 2018 02:41:03 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '19043',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '4NEL4TCFP924TGIQ9CEHBNLQIJVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '1463069302'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.scan(Limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'S9DVRNQQTQOH7MJBEJFMMFK1CVVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Sun, 11 Nov 2018 02:14:05 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'S9DVRNQQTQOH7MJBEJFMMFK1CVVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2745614147'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = {\n",
    "    'id': 'hogehoge', \n",
    "    'test': 'hello22',\n",
    "    'foo': 'bar'\n",
    "}\n",
    "\n",
    "table.put_item(Item=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6JVV7UUUI78A7CB6BSLKHFJAR7VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Sun, 11 Nov 2018 02:16:13 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '2',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '6JVV7UUUI78A7CB6BSLKHFJAR7VV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2745614147'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.delete_item(Key={'id': 'hogehoge'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {\n",
    "    'id': str(uuid.uuid1()),\n",
    "    'text': data['text'],\n",
    "    'checked': False,\n",
    "    'createdAt': timestamp,\n",
    "    'updatedAt': timestamp,\n",
    "}\n",
    "\n",
    "# write the todo to the database\n",
    "table.put_item(Item=item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
